{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Human Results"
      ],
      "metadata": {
        "id": "fsXGQOyD-o39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/Experiment_results - Humans.csv')\n",
        "\n",
        "# Function to perform analysis\n",
        "def analyze_data(data, metrics_pos, metrics_neg):\n",
        "    for thread in ['T1', 'T2']:\n",
        "        print(f\"\\nDescriptive Statistics for Overall Scores - Thread {thread}:\\n\",\n",
        "              data[data['Comment ID'] == thread]['Overall Score (1-5)'].describe())\n",
        "    print(\"\\nPositive Metrics Frequency:\\n\", data[metrics_pos].sum(),\n",
        "          \"\\nNegative Metrics Frequency:\\n\", data[metrics_neg].sum())\n",
        "    comparison = pd.DataFrame({f'Thread {i+1}': data[data['Comment ID'] == f'T{i+1}'][metrics_pos + metrics_neg].sum()\n",
        "                              for i in range(2)})\n",
        "    print(\"\\nThread Comparison:\\n\", comparison)\n",
        "    print(\"\\nCorrelation with Overall Score:\\n\",\n",
        "          data[metrics_pos + metrics_neg + ['Overall Score (1-5)']].corr()['Overall Score (1-5)'].drop('Overall Score (1-5)'))\n",
        "\n",
        "# Define metrics\n",
        "positive_metrics = ['Politeness', 'Empathy', 'Balanced Participation', 'Respectful Disagreement',\n",
        "                    'Engagement', 'Purpose of Responses', 'Persuasiveness', 'Argument Clarity',\n",
        "                    'Simple Language (Positive)', 'Complex Language (Positive)', 'Sarcasm (Positive)',\n",
        "                    'Information Sharing (Positive)']\n",
        "negative_metrics = ['Simple Language (Negative)', 'Complex Language (Negative)', 'Sarcasm (Negative)',\n",
        "                    'Information Sharing (Negative)', 'Rudeness', 'Dominating Behaviour',\n",
        "                    'Hostile disaggreement', 'Lack of engagement', 'Lack of meaningful contribution',\n",
        "                    'Lack persuasiveness', 'Argument unclarity']\n",
        "\n",
        "# Run analysis\n",
        "analyze_data(data, positive_metrics, negative_metrics)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'Overall Scores Statistics': pd.DataFrame({f'Thread {i+1}': data[data['Comment ID'] == f'T{i+1}']['Overall Score (1-5)'].describe() for i in range(2)}),\n",
        "    'Positive Metrics Frequency': data[positive_metrics].sum(),\n",
        "    'Negative Metrics Frequency': data[negative_metrics].sum(),\n",
        "    'Thread Comparison': pd.DataFrame({f'Thread {i+1}': data[data['Comment ID'] == f'T{i+1}'][positive_metrics + negative_metrics].sum() for i in range(2)}),\n",
        "    'Correlation with Overall Score': data[positive_metrics + negative_metrics + ['Overall Score (1-5)']].corr()['Overall Score (1-5)'].drop('Overall Score (1-5)')\n",
        "}\n",
        "\n",
        "with pd.ExcelWriter('/content/experiment_analysis_results.xlsx') as writer:\n",
        "    for sheet, df in results.items():\n",
        "        df.to_excel(writer, sheet_name=sheet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F41fHaJ--r42",
        "outputId": "2df645c4-5a51-43e7-de8f-eb313315375f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive Statistics for Overall Scores - Thread T1:\n",
            " count    37.000000\n",
            "mean      2.918919\n",
            "std       0.795067\n",
            "min       2.000000\n",
            "25%       2.000000\n",
            "50%       3.000000\n",
            "75%       4.000000\n",
            "max       4.000000\n",
            "Name: Overall Score (1-5), dtype: float64\n",
            "\n",
            "Descriptive Statistics for Overall Scores - Thread T2:\n",
            " count    37.000000\n",
            "mean      3.783784\n",
            "std       0.583816\n",
            "min       3.000000\n",
            "25%       3.000000\n",
            "50%       4.000000\n",
            "75%       4.000000\n",
            "max       5.000000\n",
            "Name: Overall Score (1-5), dtype: float64\n",
            "\n",
            "Positive Metrics Frequency:\n",
            " Politeness                        38\n",
            "Empathy                           20\n",
            "Balanced Participation            29\n",
            "Respectful Disagreement           30\n",
            "Engagement                        21\n",
            "Purpose of Responses              28\n",
            "Persuasiveness                    14\n",
            "Argument Clarity                  31\n",
            "Simple Language (Positive)        36\n",
            "Complex Language (Positive)        2\n",
            "Sarcasm (Positive)                11\n",
            "Information Sharing (Positive)    35\n",
            "dtype: int64 \n",
            "Negative Metrics Frequency:\n",
            " Simple Language (Negative)          7\n",
            "Complex Language (Negative)         4\n",
            "Sarcasm (Negative)                 14\n",
            "Information Sharing (Negative)      7\n",
            "Rudeness                           12\n",
            "Dominating Behaviour               10\n",
            "Hostile disaggreement               4\n",
            "Lack of engagement                 13\n",
            "Lack of meaningful contribution    15\n",
            "Lack persuasiveness                15\n",
            "Argument unclarity                  3\n",
            "dtype: int64\n",
            "\n",
            "Thread Comparison:\n",
            "                                  Thread 1  Thread 2\n",
            "Politeness                              7        31\n",
            "Empathy                                 1        19\n",
            "Balanced Participation                  6        23\n",
            "Respectful Disagreement                 8        22\n",
            "Engagement                              5        16\n",
            "Purpose of Responses                   13        15\n",
            "Persuasiveness                          3        11\n",
            "Argument Clarity                       10        21\n",
            "Simple Language (Positive)             13        23\n",
            "Complex Language (Positive)             0         2\n",
            "Sarcasm (Positive)                      7         4\n",
            "Information Sharing (Positive)         12        23\n",
            "Simple Language (Negative)              3         4\n",
            "Complex Language (Negative)             4         0\n",
            "Sarcasm (Negative)                     12         2\n",
            "Information Sharing (Negative)          5         2\n",
            "Rudeness                               12         0\n",
            "Dominating Behaviour                    9         1\n",
            "Hostile disaggreement                   4         0\n",
            "Lack of engagement                     11         2\n",
            "Lack of meaningful contribution        11         4\n",
            "Lack persuasiveness                    11         4\n",
            "Argument unclarity                      2         1\n",
            "\n",
            "Correlation with Overall Score:\n",
            " Politeness                         0.587200\n",
            "Empathy                            0.448340\n",
            "Balanced Participation             0.266078\n",
            "Respectful Disagreement            0.455888\n",
            "Engagement                         0.502457\n",
            "Purpose of Responses               0.211297\n",
            "Persuasiveness                     0.428041\n",
            "Argument Clarity                   0.441817\n",
            "Simple Language (Positive)         0.311135\n",
            "Complex Language (Positive)       -0.072062\n",
            "Sarcasm (Positive)                 0.099811\n",
            "Information Sharing (Positive)     0.456412\n",
            "Simple Language (Negative)         0.087540\n",
            "Complex Language (Negative)       -0.250442\n",
            "Sarcasm (Negative)                -0.293776\n",
            "Information Sharing (Negative)    -0.253404\n",
            "Rudeness                          -0.415799\n",
            "Dominating Behaviour              -0.316842\n",
            "Hostile disaggreement             -0.323985\n",
            "Lack of engagement                -0.330702\n",
            "Lack of meaningful contribution   -0.507575\n",
            "Lack persuasiveness               -0.342110\n",
            "Argument unclarity                -0.257516\n",
            "Name: Overall Score (1-5), dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and round the correlations\n",
        "correlations = data[positive_metrics + negative_metrics + ['Overall Score (1-5)']].corr()['Overall Score (1-5)'].drop('Overall Score (1-5)').round(2)\n",
        "\n",
        "# Print the results\n",
        "print(\"Correlation with Overall Score:\\n\", correlations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDB2U-KxaaRc",
        "outputId": "8b755596-20af-496c-deda-1667c417ae7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation with Overall Score:\n",
            " Politeness                         0.59\n",
            "Empathy                            0.45\n",
            "Balanced Participation             0.27\n",
            "Respectful Disagreement            0.46\n",
            "Engagement                         0.50\n",
            "Purpose of Responses               0.21\n",
            "Persuasiveness                     0.43\n",
            "Argument Clarity                   0.44\n",
            "Simple Language (Positive)         0.31\n",
            "Complex Language (Positive)       -0.07\n",
            "Sarcasm (Positive)                 0.10\n",
            "Information Sharing (Positive)     0.46\n",
            "Simple Language (Negative)         0.09\n",
            "Complex Language (Negative)       -0.25\n",
            "Sarcasm (Negative)                -0.29\n",
            "Information Sharing (Negative)    -0.25\n",
            "Rudeness                          -0.42\n",
            "Dominating Behaviour              -0.32\n",
            "Hostile disaggreement             -0.32\n",
            "Lack of engagement                -0.33\n",
            "Lack of meaningful contribution   -0.51\n",
            "Lack persuasiveness               -0.34\n",
            "Argument unclarity                -0.26\n",
            "Name: Overall Score (1-5), dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Results"
      ],
      "metadata": {
        "id": "09Gz-KS9bH6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV data\n",
        "df = pd.read_csv('/content/Experiment_results - LLMs.csv')\n",
        "\n",
        "# 1. Overall Score Comparison: Calculate average and standard deviation of overall scores by model\n",
        "overall_score_stats = df.groupby('Model')['Overall Score (1-5)'].agg(['mean', 'std']).reset_index()\n",
        "print(\"Overall Score Stats:\\n\", overall_score_stats)\n",
        "\n",
        "# 2. Metric Frequency Comparison: Count how often each metric is flagged (non-zero) by each model\n",
        "\n",
        "\n",
        "metrics = ['Politeness',\n",
        "       'Empathy', 'Balanced Participation', 'Respectful Disagreement',\n",
        "       'Engagement', 'Purpose of Responses', 'Persuasiveness',\n",
        "       'Argument Clarity', 'Simple Language (Positive)',\n",
        "       'Complex Language (Positive)', 'Sarcasm (Positive)',\n",
        "       'Information Sharing (Positive)', 'Simple Language (Negative)',\n",
        "       'Complex Language (Negative)', 'Sarcasm (Negative)',\n",
        "       'Information Sharing (Negative)', 'Rudeness', 'Dominating Behaviour',\n",
        "       'Hostile disagreement', 'Lack of engagement',\n",
        "       'Lack of meaningful contribution', 'Lack persuasiveness',\n",
        "       'Argument unclarity']  # You can add more metrics as needed\n",
        "metric_flags = df.groupby('Model')[metrics].sum().reset_index()\n",
        "print(\"\\nMetric Flags by Model:\\n\", metric_flags)\n",
        "\n",
        "# 3. Inter-model Comparison: Check consistency of scores for each model (standard deviation across runs)\n",
        "inter_model_consistency = df.groupby(['Model', 'Comment ID'])['Overall Score (1-5)'].agg(['mean', 'std']).reset_index()\n",
        "print(\"\\nInter-model Score Consistency:\\n\", inter_model_consistency)\n",
        "\n",
        "# save the results to CSV files\n",
        "overall_score_stats.to_csv('overall_score_stats.csv', index=False)\n",
        "metric_flags.to_csv('metric_flags_by_model.csv', index=False)\n",
        "inter_model_consistency.to_csv('inter_model_consistency.csv', index=False)\n",
        "\n",
        "# Further analysis for qualitative data (Additional Comments) can be done manually or using natural language processing techniques.\n"
      ],
      "metadata": {
        "id": "qTEILx5TbLoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfa815e-d189-4ebd-f211-d194a31c4427"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Score Stats:\n",
            "                   Model  mean       std\n",
            "0               1o-mini   4.0  0.471405\n",
            "1            1o-preview   4.0  0.471405\n",
            "2                gpt-4o   3.5  0.527046\n",
            "3           gpt-4o-mini   4.0  0.000000\n",
            "4  mistral-large-latest   4.0  0.000000\n",
            "5     open-mistral-nemo   3.0  0.000000\n",
            "\n",
            "Metric Flags by Model:\n",
            "                   Model  Politeness  Empathy  Balanced Participation  \\\n",
            "0               1o-mini          10        9                       5   \n",
            "1            1o-preview           9        9                       7   \n",
            "2                gpt-4o          10        5                       4   \n",
            "3           gpt-4o-mini          10       10                       9   \n",
            "4  mistral-large-latest          10       10                      10   \n",
            "5     open-mistral-nemo          10        5                       0   \n",
            "\n",
            "   Respectful Disagreement  Engagement  Purpose of Responses  Persuasiveness  \\\n",
            "0                        8          10                    10               8   \n",
            "1                        8          10                    10               7   \n",
            "2                       10          10                     9               3   \n",
            "3                       10          10                    10               8   \n",
            "4                       10          10                    10              10   \n",
            "5                       10           9                     9               3   \n",
            "\n",
            "   Argument Clarity  Simple Language (Positive)  ...  \\\n",
            "0                10                          10  ...   \n",
            "1                10                          10  ...   \n",
            "2                10                          10  ...   \n",
            "3                10                          10  ...   \n",
            "4                10                          10  ...   \n",
            "5                 9                          10  ...   \n",
            "\n",
            "   Complex Language (Negative)  Sarcasm (Negative)  \\\n",
            "0                            0                   3   \n",
            "1                            0                   5   \n",
            "2                            0                   3   \n",
            "3                            0                   2   \n",
            "4                            0                   0   \n",
            "5                            0                   0   \n",
            "\n",
            "   Information Sharing (Negative)  Rudeness  Dominating Behaviour  \\\n",
            "0                               0         4                     0   \n",
            "1                               0         3                     1   \n",
            "2                               0         0                     0   \n",
            "3                               0         0                     0   \n",
            "4                               0         0                     0   \n",
            "5                               1         4                     5   \n",
            "\n",
            "   Hostile disagreement  Lack of engagement  Lack of meaningful contribution  \\\n",
            "0                     3                   0                                0   \n",
            "1                     1                   0                                0   \n",
            "2                     0                   0                                1   \n",
            "3                     0                   0                                0   \n",
            "4                     0                   0                                0   \n",
            "5                     1                   2                                2   \n",
            "\n",
            "   Lack persuasiveness  Argument unclarity  \n",
            "0                    0                   0  \n",
            "1                    1                   0  \n",
            "2                    6                   0  \n",
            "3                    2                   0  \n",
            "4                    0                   0  \n",
            "5                    7                   1  \n",
            "\n",
            "[6 rows x 24 columns]\n",
            "\n",
            "Inter-model Score Consistency:\n",
            "                    Model Comment ID  mean       std\n",
            "0                1o-mini         T1   3.8  0.447214\n",
            "1                1o-mini         T2   4.2  0.447214\n",
            "2             1o-preview         T1   3.8  0.447214\n",
            "3             1o-preview         T2   4.2  0.447214\n",
            "4                 gpt-4o         T1   3.0  0.000000\n",
            "5                 gpt-4o         T2   4.0  0.000000\n",
            "6            gpt-4o-mini         T1   4.0  0.000000\n",
            "7            gpt-4o-mini         T2   4.0  0.000000\n",
            "8   mistral-large-latest         T1   4.0  0.000000\n",
            "9   mistral-large-latest         T2   4.0  0.000000\n",
            "10     open-mistral-nemo         T1   3.0  0.000000\n",
            "11     open-mistral-nemo         T2   3.0  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV data\n",
        "# Replace with your actual file paths\n",
        "llm_data = pd.read_csv('/content/Experiment_results - LLMs.csv')\n",
        "human_data = pd.read_csv('/content/Experiment_results - Humans.csv')\n",
        "\n",
        "# Define the positive and negative metrics lists\n",
        "positive_metrics = ['Politeness', 'Empathy', 'Balanced Participation', 'Respectful Disagreement',\n",
        "                    'Engagement', 'Purpose of Responses', 'Persuasiveness', 'Argument Clarity',\n",
        "                    'Simple Language (Positive)', 'Complex Language (Positive)', 'Sarcasm (Positive)',\n",
        "                    'Information Sharing (Positive)']\n",
        "negative_metrics = ['Simple Language (Negative)', 'Complex Language (Negative)', 'Sarcasm (Negative)',\n",
        "                    'Information Sharing (Negative)', 'Rudeness', 'Dominating Behaviour',\n",
        "                    'Hostile disagreement', 'Lack of engagement', 'Lack of meaningful contribution',\n",
        "                    'Lack persuasiveness', 'Argument unclarity']\n",
        "\n",
        "# Step 1: Calculate the frequency of each positive and negative metric for humans\n",
        "human_pos_freq = human_data[positive_metrics].sum().nlargest(3)\n",
        "human_neg_freq = human_data[negative_metrics].sum().nlargest(3)\n",
        "\n",
        "# Display top 3 positive and top 3 negative metrics based on human evaluations\n",
        "print(\"Top 3 Positive Metrics by Human Evaluators:\\n\", human_pos_freq)\n",
        "print(\"\\nTop 3 Negative Metrics by Human Evaluators:\\n\", human_neg_freq)\n",
        "\n",
        "# Step 2: Filter the LLM data to include only the top human metrics\n",
        "top_human_metrics = list(human_pos_freq.index) + list(human_neg_freq.index)\n",
        "llm_filtered_data = llm_data[['Model'] + top_human_metrics]\n",
        "\n",
        "# Save the filtered LLM data to a CSV if needed\n",
        "llm_filtered_data.to_csv('filtered_llm_metrics.csv', index=False)\n",
        "\n",
        "# Step 3: Count how many times each LLM flagged the top metrics and print the results\n",
        "print(\"\\nNumber of Times Each LLM Flagged Top Human Metrics:\")\n",
        "for metric in top_human_metrics:\n",
        "    counts = llm_filtered_data.groupby('Model')[metric].sum()\n",
        "    print(f\"\\nMetric: {metric}\")\n",
        "    print(counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVZQnBWUicxb",
        "outputId": "8067d1bb-ebf8-4f73-cd34-7a8d5da6fe35"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Positive Metrics by Human Evaluators:\n",
            " Politeness                        38.0\n",
            "Simple Language (Positive)        36.0\n",
            "Information Sharing (Positive)    35.0\n",
            "dtype: float64\n",
            "\n",
            "Top 3 Negative Metrics by Human Evaluators:\n",
            " Lack of meaningful contribution    15.0\n",
            "Lack persuasiveness                15.0\n",
            "Sarcasm (Negative)                 14.0\n",
            "dtype: float64\n",
            "\n",
            "Number of Times Each LLM Flagged Top Human Metrics:\n",
            "\n",
            "Metric: Politeness\n",
            "Model\n",
            "1o-mini                 10\n",
            "1o-preview               9\n",
            "gpt-4o                  10\n",
            "gpt-4o-mini             10\n",
            "mistral-large-latest    10\n",
            "open-mistral-nemo       10\n",
            "Name: Politeness, dtype: int64\n",
            "\n",
            "Metric: Simple Language (Positive)\n",
            "Model\n",
            "1o-mini                 10\n",
            "1o-preview              10\n",
            "gpt-4o                  10\n",
            "gpt-4o-mini             10\n",
            "mistral-large-latest    10\n",
            "open-mistral-nemo       10\n",
            "Name: Simple Language (Positive), dtype: int64\n",
            "\n",
            "Metric: Information Sharing (Positive)\n",
            "Model\n",
            "1o-mini                 10\n",
            "1o-preview              10\n",
            "gpt-4o                  10\n",
            "gpt-4o-mini             10\n",
            "mistral-large-latest    10\n",
            "open-mistral-nemo        9\n",
            "Name: Information Sharing (Positive), dtype: int64\n",
            "\n",
            "Metric: Lack of meaningful contribution\n",
            "Model\n",
            "1o-mini                 0\n",
            "1o-preview              0\n",
            "gpt-4o                  1\n",
            "gpt-4o-mini             0\n",
            "mistral-large-latest    0\n",
            "open-mistral-nemo       2\n",
            "Name: Lack of meaningful contribution, dtype: int64\n",
            "\n",
            "Metric: Lack persuasiveness\n",
            "Model\n",
            "1o-mini                 0\n",
            "1o-preview              1\n",
            "gpt-4o                  6\n",
            "gpt-4o-mini             2\n",
            "mistral-large-latest    0\n",
            "open-mistral-nemo       7\n",
            "Name: Lack persuasiveness, dtype: int64\n",
            "\n",
            "Metric: Sarcasm (Negative)\n",
            "Model\n",
            "1o-mini                 3\n",
            "1o-preview              5\n",
            "gpt-4o                  3\n",
            "gpt-4o-mini             2\n",
            "mistral-large-latest    0\n",
            "open-mistral-nemo       0\n",
            "Name: Sarcasm (Negative), dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV data\n",
        "# Replace with your actual file paths\n",
        "llm_data = pd.read_csv('/content/Experiment_results - LLMs.csv')\n",
        "human_data = pd.read_csv('/content/Experiment_results - Humans.csv')\n",
        "\n",
        "# Define the positive and negative metrics lists\n",
        "positive_metrics = ['Politeness', 'Empathy', 'Balanced Participation', 'Respectful Disagreement',\n",
        "                    'Engagement', 'Purpose of Responses', 'Persuasiveness', 'Argument Clarity',\n",
        "                    'Simple Language (Positive)', 'Complex Language (Positive)', 'Sarcasm (Positive)',\n",
        "                    'Information Sharing (Positive)']\n",
        "negative_metrics = ['Simple Language (Negative)', 'Complex Language (Negative)', 'Sarcasm (Negative)',\n",
        "                    'Information Sharing (Negative)', 'Rudeness', 'Dominating Behaviour',\n",
        "                    'Hostile disagreement', 'Lack of engagement', 'Lack of meaningful contribution',\n",
        "                    'Lack persuasiveness', 'Argument unclarity']\n",
        "\n",
        "# Step 1: Calculate the frequency of each positive and negative metric for humans\n",
        "human_pos_freq = human_data[positive_metrics].sum().nlargest(3)\n",
        "human_neg_freq = human_data[negative_metrics].sum().nlargest(3)\n",
        "\n",
        "# Display top 3 positive and top 3 negative metrics based on human evaluations\n",
        "print(\"Top 3 Positive Metrics by Human Evaluators:\\n\", human_pos_freq)\n",
        "print(\"\\nTop 3 Negative Metrics by Human Evaluators:\\n\", human_neg_freq)\n",
        "\n",
        "# Step 2: Filter the LLM data to include only the top human metrics\n",
        "top_human_metrics = list(human_pos_freq.index) + list(human_neg_freq.index)\n",
        "llm_filtered_data = llm_data[['Model'] + top_human_metrics]\n",
        "\n",
        "# Step 3: Aggregate the metric flags for each model and calculate percentages\n",
        "llm_aggregated = llm_filtered_data.groupby('Model')[top_human_metrics].sum()\n",
        "\n",
        "# Calculate percentage of flags for each LLM\n",
        "max_llm_flag = 10  # Max possible flags for LLMs\n",
        "llm_percentage_data = (llm_aggregated / max_llm_flag) * 100\n",
        "\n",
        "# Calculate percentage of flags for humans\n",
        "max_human_flag = 74  # Max possible flags for humans\n",
        "human_percentage_data = (human_data[top_human_metrics].sum() / max_human_flag) * 100\n",
        "\n",
        "# Print LLM percentages\n",
        "print(\"\\nPercentage of Top Metrics Flagged by Each LLM Model:\\n\", llm_percentage_data)\n",
        "\n",
        "# Print Human percentages\n",
        "print(\"\\nPercentage of Top Metrics Flagged by Human Evaluators:\\n\", human_percentage_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgBa6OMXuqOm",
        "outputId": "e1814a7d-da6f-4fcc-e6d0-20c8f9f72014"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Positive Metrics by Human Evaluators:\n",
            " Politeness                        38.0\n",
            "Simple Language (Positive)        36.0\n",
            "Information Sharing (Positive)    35.0\n",
            "dtype: float64\n",
            "\n",
            "Top 3 Negative Metrics by Human Evaluators:\n",
            " Lack of meaningful contribution    15.0\n",
            "Lack persuasiveness                15.0\n",
            "Sarcasm (Negative)                 14.0\n",
            "dtype: float64\n",
            "\n",
            "Percentage of Top Metrics Flagged by Each LLM Model:\n",
            "                       Politeness  Simple Language (Positive)  \\\n",
            "Model                                                          \n",
            "1o-mini                    100.0                       100.0   \n",
            "1o-preview                  90.0                       100.0   \n",
            "gpt-4o                     100.0                       100.0   \n",
            "gpt-4o-mini                100.0                       100.0   \n",
            "mistral-large-latest       100.0                       100.0   \n",
            "open-mistral-nemo          100.0                       100.0   \n",
            "\n",
            "                      Information Sharing (Positive)  \\\n",
            "Model                                                  \n",
            "1o-mini                                        100.0   \n",
            "1o-preview                                     100.0   \n",
            "gpt-4o                                         100.0   \n",
            "gpt-4o-mini                                    100.0   \n",
            "mistral-large-latest                           100.0   \n",
            "open-mistral-nemo                               90.0   \n",
            "\n",
            "                      Lack of meaningful contribution  Lack persuasiveness  \\\n",
            "Model                                                                        \n",
            "1o-mini                                           0.0                  0.0   \n",
            "1o-preview                                        0.0                 10.0   \n",
            "gpt-4o                                           10.0                 60.0   \n",
            "gpt-4o-mini                                       0.0                 20.0   \n",
            "mistral-large-latest                              0.0                  0.0   \n",
            "open-mistral-nemo                                20.0                 70.0   \n",
            "\n",
            "                      Sarcasm (Negative)  \n",
            "Model                                     \n",
            "1o-mini                             30.0  \n",
            "1o-preview                          50.0  \n",
            "gpt-4o                              30.0  \n",
            "gpt-4o-mini                         20.0  \n",
            "mistral-large-latest                 0.0  \n",
            "open-mistral-nemo                    0.0  \n",
            "\n",
            "Percentage of Top Metrics Flagged by Human Evaluators:\n",
            " Politeness                         51.351351\n",
            "Simple Language (Positive)         48.648649\n",
            "Information Sharing (Positive)     47.297297\n",
            "Lack of meaningful contribution    20.270270\n",
            "Lack persuasiveness                20.270270\n",
            "Sarcasm (Negative)                 18.918919\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Human mean scores for Thread 1 and Thread 2\n",
        "human_means = np.array([2.92, 3.78])\n",
        "\n",
        "# LLM mean scores for each model by thread\n",
        "llm_means = {\n",
        "    '1o-mini': [3.8, 4.2],\n",
        "    '1o-preview': [3.8, 4.2],\n",
        "    'GPT-4o': [3.0, 4.0],\n",
        "    'GPT-4o-mini': [4.0, 4.0],\n",
        "    'Mistral Large': [4.0, 4.0],\n",
        "    'Mistral Nemo': [3.0, 3.0]\n",
        "}\n",
        "\n",
        "# Calculate MAE for each model with human scores\n",
        "mae_results = {}\n",
        "for model, scores in llm_means.items():\n",
        "    scores_array = np.array(scores)\n",
        "    mae = np.mean(np.abs(human_means - scores_array))\n",
        "    mae_results[model] = mae\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "mae_results_df = pd.DataFrame.from_dict(mae_results, orient='index', columns=['Mean Absolute Error (MAE)'])\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nMean Absolute Error (MAE) of LLM Model Scores with Human Scores:\\n\", mae_results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vHSl9k_z2s_",
        "outputId": "f1e28e87-ffac-4109-da16-6f9642c26f8a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Absolute Error (MAE) of LLM Model Scores with Human Scores:\n",
            "                Mean Absolute Error (MAE)\n",
            "1o-mini                             0.65\n",
            "1o-preview                          0.65\n",
            "GPT-4o                              0.15\n",
            "GPT-4o-mini                         0.65\n",
            "Mistral Large                       0.65\n",
            "Mistral Nemo                        0.43\n"
          ]
        }
      ]
    }
  ]
}